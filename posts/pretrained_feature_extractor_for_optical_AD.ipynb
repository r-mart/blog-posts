{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained feature extractor for optical anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "import timm\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "import rootutils\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import save_plot_from_notbook_for_jekyll, bokeh_notebook_setup, save_plot_from_notebook_to_html\n",
    "from src.visualization.image import plot_img_rgba, add_bboxes_on_img, add_seg_on_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/transistor\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- task similar to previous post\n",
    "  - we don't have regular patterns anymore\n",
    "  - we want to have a method that can be applied to several tasks\n",
    "- previous research resulted in high effectiveness for features extracted from Deep Learning models pre-trained on ImageNet\n",
    "  - reference to SPADE, Gaussian AD, PaDim and PatchCore\n",
    "- we will be following the approach of these papers but change a few components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Like in the previous post, we will use the [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad) which you can download from the website.\n",
    "The dataset contains 15 different categories. For the examples in this post we will use the 'Transistor' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a normal example without anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"train/good/000.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in contrast an anomalous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"test/bent_lead/000.png\"\n",
    "seg_path = data_path / \"ground_truth/bent_lead/000_mask.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "p = add_seg_on_img(p, seg)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Like in the PaDim or PatchCore paper we are going to extract features for each image patch of the training set using a neural network architecture for vision tasks pre-trained on the ImageNet dataset. The patch size is determined by our choice for the network layer.\n",
    "To do the feature extraction we use the PyTorch `feature_extraction` package [based on Torch FX](https://pytorch.org/blog/FX-feature-extraction-torchvision/).\n",
    "The goal of this post is to demonstrate the principle rather than optimizing our approach to the dataset. Hence, we will simplify many steps compared to the paper.\n",
    "\n",
    "For the backbone we pick ResNet 34 as it is simple to use and doesn't require much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\"resnet34\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the papers features from several layers were combined. To keep it simple, we will use only one layer.\n",
    "To see the available layer names for feature extraction you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `train_nodes` or `eval_nodes`, you will see that ResNet 34 has 4 main blocks. If you just want to pick the last node of a block, the feature_extraction module allows you to use truncated node names. Here we will be using `'layer3'` to get the last node of all the `layer3.x.ops` nodes. We choose layer 3 as a compromise between having expressive high-level features but still a somewhat high spatial feature map resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(backbone, return_nodes=[\"layer3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the paper, we fix the weights to the pre-trained ImageNet weights. Hence, we can turn off gradient computation to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify experimenting with different configurations, we use a Config object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    img_shape = (512, 512)  # height, width (multiple of 16)\n",
    "    batch_size = 4\n",
    "    num_workers: int = 2  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    red_factor = 16  # spatial reduction factor (depends on the chosen ResNet layer)\n",
    "    n_feats = 256  # number of features (depends on the chosen ResNet layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: shorten the explanation. I can't assume ppl have read the paper\n",
    "\n",
    "To save the features we will follow the memory bank approach from PatchCore. This means we will save the extracted features into a large array without linking them to the original patch location. This is in contrast to the PaDim approach. The advantage is that our approach becomes more robust to rotations, translations and other variations of the objects in the dataset. The disadvantage is that the number of feature vectors we have to compare each patch to becomes quite large. \n",
    "TODO brief calculation: N images * Height * Width of feature maps\n",
    "To use a method like nearest neighbor lookup like in the PatchCore paper, this requires tricks like the coreset reduction. \n",
    "We will get around this by choosing a different anomaly detection approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a pyTorch Dataset object to hold the image data and specify the necessary transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_path: os.PathLike, transforms: Optional[A.Compose] = None):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "\n",
    "default_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(Config.img_shape[0], Config.img_shape[1]),\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = TrainDataset(train_path, transforms=default_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we create the DataLoader object to feed our data to the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function containing the logic to call the feature extractor with a batch of images and collect the resulting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature_dict = extractor(imgs)\n",
    "\n",
    "    layer = list(feature_dict.keys())[0]\n",
    "\n",
    "    feats = feature_dict[layer]\n",
    "\n",
    "    feats = feats.cpu().numpy()\n",
    "    feats = np.transpose(feats, (0, 2, 3, 1))\n",
    "    feats = feats.reshape(-1, cfg.n_feats)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put everything together to compute the feature memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = Config.img_shape[:2]\n",
    "\n",
    "# ResNet resolution reduction of layer 2\n",
    "h_layer = math.ceil(h / Config.red_factor)\n",
    "w_layer = math.ceil(w / Config.red_factor)\n",
    "\n",
    "memory_bank_size = len(train_ds) * h_layer * w_layer\n",
    "memory_bank = np.empty((memory_bank_size, Config.n_feats), dtype=np.float32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_mem = 0\n",
    "\n",
    "for i, imgs in enumerate(train_dl):\n",
    "    n_samples = imgs.shape[0]\n",
    "\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    memory_bank[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the memory bank shape, we see that it contains more than 2 million feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in the next step in which we want to compute anomaly scores for each patch of a test image by comparing with the memory bank we need an anomaly detection approach which can deal with such a large number of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_aoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
