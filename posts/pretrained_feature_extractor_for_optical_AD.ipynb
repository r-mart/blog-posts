{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained feature extractor for optical anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import list_models, get_model\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "import bokeh\n",
    "\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import (\n",
    "    save_plot_from_notbook_for_jekyll,\n",
    "    bokeh_notebook_setup,\n",
    ")\n",
    "from src.visualization.image import (\n",
    "    plot_img_rgba,\n",
    "    add_seg_on_img,\n",
    "    add_score_map_on_img,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()\n",
    "\n",
    "# make random number generator repeatable\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/metal_nut\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post we will look at an approach to detect anomalies in images. The goal is that it should be applicable to common automatic optical inspection scenarios in manufacturing like for example in the semiconductor industry.\n",
    "Often for optical inspection in manufacturing companies can provide plenty of example images for products without any anomalies (normal samples). However, as a lot of effort is put into optimizing the manufacturing processes, example images of defect products (anomalies) are scarce.\n",
    "Furthermore, it is often difficult to predict in advance what kind of defects may appear. This makes common supervised image classification or segmentation approaches unfeasible.\n",
    "We will address this scenario with an anomaly detection approach which uses only normal samples for training and is able to detect any deviations from the normal case on a pixel level.\n",
    "Previous research in this direction has demonstrated a high effectiveness of features extracted from Deep Learning models pre-trained on the ImageNet dataset. See for example the SOTA approaches on the [Anomaly Detection on MVTec AD benchmark](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad). The approach here is in particular inspired by the papers:\n",
    "\n",
    "- [SPADE - Sub-Image Anomaly Detection with Deep Pyramid Correspondences](https://paperswithcode.com/paper/sub-image-anomaly-detection-with-deep-pyramid)\n",
    "- [Gaussian-AD - Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection](https://paperswithcode.com/paper/modeling-the-distribution-of-normal-data-in)\n",
    "- [PaDiM - a Patch Distribution Modeling Framework for Anomaly Detection and Localization](https://paperswithcode.com/paper/padim-a-patch-distribution-modeling-framework)\n",
    "- [PatchCore - Towards Total Recall in Industrial Anomaly Detection](https://paperswithcode.com/paper/towards-total-recall-in-industrial-anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Like in the previous post, we will use the [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad) which you can download from the website.\n",
    "The dataset contains 15 different categories. For the examples in this post we will use the 'Metal Nut' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a normal example without anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"train/good/000.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in contrast an anomalous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"test/bent/000.png\"\n",
    "seg_path = data_path / \"ground_truth/bent/000_mask.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "p = add_seg_on_img(p, seg)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Like in the PaDim or PatchCore paper we are going to extract features for each image patch of the training set using a neural network architecture for vision tasks pre-trained on the ImageNet dataset. The patch size is determined by our choice for the network layer. Earlier layers in the network will in general yield smaller patch sizes.<br>\n",
    "To do the feature extraction we use the PyTorch `feature_extraction` package [based on Torch FX](https://pytorch.org/blog/FX-feature-extraction-torchvision/).\n",
    "The goal of this post is to demonstrate the principle rather than optimizing our approach to the dataset. Hence, we will simplify some steps compared to the papers.\n",
    "\n",
    "For the backbone we pick the [ConvNeXt architecture](https://arxiv.org/abs/2201.03545)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = get_model(\"convnext_base\", weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the papers, features from several layers were combined. To keep it simple, we will use only one layer.\n",
    "To see the available layer names for feature extraction you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `train_nodes` or `eval_nodes`, you will see that ConvNeXt base has 7 main feature blocks. If you just want to pick the last node of a block, the feature_extraction module allows you to use truncated node names. We will use `'features.3'` to get the last node of all the `features.3.x.ops` nodes. We choose layer 3 as a compromise between having expressive high-level features but still a somewhat high spatial feature map resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [\"features.3\"]\n",
    "feature_extractor = create_feature_extractor(backbone, return_nodes=layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the papers, we fix the weights to the pre-trained ImageNet weights. Hence, we can turn off gradient computation to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the PatchCore paper to apply an average pooling layer to the features extracted for each layer. This should give the extracted features more context from their local neighborhood. The motivation is that sometimes by looking at a single patch it is impossible to determine whether a structure is an anomaly. If, however, you also look at the surrounding patches it often becomes more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCoreModel(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.patch_layer = torch.nn.AvgPool2d(3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_dict = self.feature_extractor(x)\n",
    "        for k, v in feature_dict.items():\n",
    "            feature_dict[k] = self.patch_layer(v)\n",
    "\n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor=PatchCoreModel(feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify experimenting with different configurations, we use a Config object. \n",
    "\n",
    "In theory, we could look up the number of features and the spatial feature reduction factor from the model source code or paper. However, it is easier to determine later. We therefore set it to 'None' for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    img_shape = (224, 224)  # height, width\n",
    "    batch_size = 4\n",
    "    num_workers: int = 2  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    red_factor = None  # spatial reduction factor (equivalent to patch size)\n",
    "    n_feats = None  # number of features (depends on the chosen layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the features we will follow the memory bank approach from PatchCore. We save the extracted features into a large array without linking them to the original patch location. This means our approach becomes more robust to rotations, translations and other spatial variations of the objects in the dataset. The disadvantage is that the number of feature vectors we have to compare each patch to becomes quite large. Therefore, to make a simple nearest neighbor lookup feasible, further steps to reduce the memory bank size are necessary. We will get around this by choosing a different anomaly detection approach later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for the feature extractor we create a pyTorch Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "        N_train: Optional[int] = None,\n",
    "    ):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if N_train is not None and len(self.img_paths) > N_train:\n",
    "            self.img_paths = random.sample(self.img_paths, N_train)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using a backbone network pre-trained on ImageNet, we need to apply the same normalization transformations as for the original backbone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "\n",
    "default_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(Config.img_shape[0], Config.img_shape[1]),\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = TrainDataset(train_path, transforms=default_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we create the DataLoader object to feed the data to the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loader defined, we can now find the number of features and spatial reduction factor simply by running the feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(train_dl))\n",
    "feats_shapes = []\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    feats_shapes.append(feature_extractor(imgs)[layer_name].shape)\n",
    "\n",
    "Config.n_feats = sum([fs[1] for fs in feats_shapes])\n",
    "Config.red_factor = Config.img_shape[0] // feats_shapes[0][2]\n",
    "\n",
    "print(\"n feats:\", Config.n_feats)\n",
    "print(\"red factor:\", Config.red_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function containing the logic to call the feature extractor with a batch of images and collect the resulting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature_dict = extractor(imgs)\n",
    "\n",
    "    layers = list(feature_dict.keys())\n",
    "\n",
    "    feats = feature_dict[layers[0]]\n",
    "    feats = feats.cpu().numpy()\n",
    "    feats = np.transpose(feats, (0, 2, 3, 1))\n",
    "    feats = feats.reshape(-1, cfg.n_feats)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put everything together to compute the feature memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = Config.img_shape[:2]\n",
    "h_layer = math.ceil(h / Config.red_factor)\n",
    "w_layer = math.ceil(w / Config.red_factor)\n",
    "\n",
    "memory_bank_size = len(train_ds) * h_layer * w_layer\n",
    "memory_bank = np.empty((memory_bank_size, Config.n_feats), dtype=np.float32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_mem = 0\n",
    "\n",
    "for i, imgs in enumerate(train_dl):\n",
    "    n_samples = imgs.shape[0]\n",
    "\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    memory_bank[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the memory bank shape, we see that it contains almost 200k feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will compute anomaly scores for each patch of a test image by comparing its feature vector with the feature vectors in the memory bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "For the anomaly detection part, we extract the features of a target image with the same model as before. Afterwards, we apply an off-the-shelf anomaly detection algorithm from the [Python Outlier Detection (PyOD) library](https://github.com/yzhao062/pyod).\n",
    "\n",
    "Side remark: I will use the terms anomaly detection and outlier detection interchangeably.\n",
    "\n",
    "After some experiments, the [LUNAR outlier detection method](https://arxiv.org/abs/2112.05355) proved to have a good performance with reasonable processing time.<br>\n",
    "Fitting the anomaly detection model on this large set of feature vectors still takes a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LUNAR(n_neighbours=5)\n",
    "clf.fit(memory_bank)\n",
    "\n",
    "# model_path = output_path / 'clf.pkl'\n",
    "# pickle.dump(clf, open(model_path, 'wb'))\n",
    "# clf = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we pick a defect image from the training data and extract its features in the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"test/bent/000.png\"\n",
    "seg_path = data_path / \"ground_truth/bent/000_mask.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "img_np = np.array(img)\n",
    "img_t = default_transforms(image=img_np)[\"image\"]\n",
    "img_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "test_feats = get_features(img_t, feature_extractor, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an anomaly score map, we reshape the features to first match the image patch locations and eventually resize it to the original image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores = clf.decision_function(test_feats)\n",
    "score_patches = np.expand_dims(ano_scores, 0)\n",
    "score_patches = score_patches.reshape(h_layer, w_layer)\n",
    "\n",
    "anomaly_map = cv2.resize(score_patches, (img.width, img.height))\n",
    "\n",
    "# apply Gaussian blur to smooth out possible resizing artifacts\n",
    "anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "\n",
    "# make anomaly scores start at 0\n",
    "anomaly_map = anomaly_map - anomaly_map.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to overlay the anomaly score map with the original defect image and to compare with the ground truth annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "p_img = plot_img_rgba(img, title=\"Image with ground truth annotation\")\n",
    "p_img = add_seg_on_img(p_img, seg)\n",
    "p_ano = plot_img_rgba(img, title=\"Image with prediction\")\n",
    "p_ano = add_score_map_on_img(p_ano, anomaly_map, alpha=0.6)\n",
    "p = bokeh.layouts.row(p_img, p_ano)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we can see how the area with the highest anomaly scores correspond to the marked ground-truth defect annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, transforms, feature_extractor, clf, cfg) -> None:\n",
    "        self.transforms = transforms\n",
    "        self.feature_extractor = feature_extractor.to(cfg.device)\n",
    "        self.clf = clf\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.h_layer = math.ceil(cfg.img_shape[0] / cfg.red_factor)\n",
    "        self.w_layer = math.ceil(cfg.img_shape[1] / cfg.red_factor)\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> np.ndarray:\n",
    "        img_np = np.array(img)\n",
    "        img_t = self.transforms(image=img_np)[\"image\"]\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "        feats = get_features(img_t, self.feature_extractor, self.cfg)\n",
    "\n",
    "        ano_scores = self.clf.decision_function(feats)\n",
    "        score_patches = np.expand_dims(ano_scores, 0)\n",
    "        score_patches = score_patches.reshape(self.h_layer, self.w_layer)\n",
    "\n",
    "        anomaly_map = cv2.resize(score_patches, (img.width, img.height))\n",
    "\n",
    "        # apply Gaussian blur to smooth out possible resizing artifacts\n",
    "        anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "        anomaly_map = anomaly_map - anomaly_map.min()\n",
    "\n",
    "        return anomaly_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our new anomaly detector on different defect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AnomalyDetector(default_transforms, feature_extractor, clf, Config)\n",
    "\n",
    "img_it = (data_path / \"test/scratch\").iterdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = next(img_it)\n",
    "seg_path = (\n",
    "    data_path\n",
    "    / \"ground_truth\"\n",
    "    / img_path.parent.name\n",
    "    / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    ")\n",
    "\n",
    "img = Image.open(img_path)\n",
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "anomaly_map = detector(img)\n",
    "\n",
    "p_img = plot_img_rgba(img)\n",
    "p_img = add_seg_on_img(p_img, seg)\n",
    "p_ano = plot_img_rgba(img)\n",
    "p_ano = add_score_map_on_img(p_ano, anomaly_map, alpha=0.6)\n",
    "p = bokeh.layouts.row(p_img, p_ano)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we test it also on unseen good images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_max_score = anomaly_map.max()\n",
    "img_it = (data_path / \"test/good\").iterdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = next(img_it)\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "anomaly_map = detector(img)\n",
    "\n",
    "p_img = plot_img_rgba(img)\n",
    "p_ano = plot_img_rgba(img)\n",
    "p_ano = add_score_map_on_img(p_ano, anomaly_map, alpha=0.6, max_score=defect_max_score)\n",
    "p = bokeh.layouts.row(p_img, p_ano)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for plotting we set the same upper limit anomaly score as for the defect image before. This visualizes more intuitively that the anomaly scores are a lot lower than before and more or less evenly distributed over the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "To quantify how well this approach works over the whole test dataset, we make anomaly score predictions over all test images and compare with the provided ground-truth annotations using the area under receiver operating characteristic curve (AUROC) metric. See [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for more details.\n",
    "\n",
    "Like in the 'training' phase, we first create a pyTorch Dataset. As the predictor class can handle already native python image objects, we don't necessarily need a DataLoader. The DataLoader would allow us to speed up the process by using batches but for this blog post we will keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "    ):\n",
    "        super(ValidationDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(\n",
    "                    gt_path / p.name / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    "                )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.gt_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if not gt_path.exists():\n",
    "            # there are no gt annotations for good cases -> all 0\n",
    "            gt = np.zeros((img.height, img.width))\n",
    "        else:\n",
    "            gt = Image.open(gt_path)\n",
    "            gt = gt.convert(\"L\")\n",
    "            gt = np.array(gt)\n",
    "            gt = gt / 255\n",
    "\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "val_ds = ValidationDataset(val_path, gt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can loop through the validation dataset and store ground truth and anomaly score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt = val_ds[0]\n",
    "\n",
    "pred_size = len(val_ds) * img.height * img.width\n",
    "preds_pix = np.empty(pred_size, dtype=np.float32)\n",
    "gts_pix = np.empty(pred_size, dtype=np.int32)\n",
    "preds_img = np.empty(len(val_ds), dtype=np.float32)\n",
    "gts_img = np.empty(len(val_ds), dtype=np.int32)\n",
    "\n",
    "i_pix = 0\n",
    "\n",
    "for i in range(len(val_ds)):\n",
    "    img, gt = val_ds[i]\n",
    "    gt = gt.astype(np.int32)\n",
    "\n",
    "    anomaly_map = detector(img)\n",
    "    n_pix = anomaly_map.shape[0] * anomaly_map.shape[1]\n",
    "\n",
    "    preds_pix[i_pix : i_pix + n_pix] = anomaly_map.reshape((-1,))\n",
    "    gts_pix[i_pix : i_pix + n_pix] = gt.reshape((-1,))\n",
    "\n",
    "    # use max score of the map as image-level anomaly score\n",
    "    preds_img[i] = anomaly_map.max()\n",
    "    # for good images gt will be all zero, for defect images max will be 1\n",
    "    gts_img[i] = gt.max()\n",
    "\n",
    "    i_pix += n_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUROC score is computed using the ground truth values and prediction scores. We compute it first for the whole image. Here, if an image contains an anomaly anywhere the ground-truth annotation is '1' for the whole image, otherwise '0'.\n",
    "To predict a single score from the anomaly maps we simply used the maximum anomaly score of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_img, tpr_img, thresholds_img = roc_curve(gts_img, preds_img)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    title=f\"ROC curve for image-wise prediction (area = {auroc_img:.5f})\",\n",
    "    x_axis_label=\"False Positive Rate\",\n",
    "    y_axis_label=\"True Positive Rate\",\n",
    ")\n",
    "p.line(fpr_img, tpr_img, line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_path = output_path / \"ROC_curve.html\"\n",
    "# save_plot_from_notbook_for_jekyll(p, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we compute the pixel-wise AUROC score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_pix, tpr_pix, thresholds_pix = roc_curve(gts_pix, preds_pix)\n",
    "auroc_pix = auc(fpr_pix, tpr_pix)\n",
    "\n",
    "print(f\"pixel-wise AUROC: {auroc_pix:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both results are pretty high and comparable to recent results on the [Anomaly Detection on MVTec AD benchmark](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad).\n",
    "\n",
    "Note however that the benchmark takes the average score for all 15 dataset categories while here we only considered the metal nut category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrated a simple approach for image anomaly detection that reaches results comparable to the SOTA on the MVTec AD dataset.\n",
    "This approach uses only normal data samples and doesn't require a conventional Deep Learning training pipeline, just a 'memorizing' of features. It can therefore be easily applied in practice, even without a powerful workstation. The main assumption is that the images are fairly similar to natural images (as this is what ImageNet was originally trained for). Furthermore, the complexity should be similar to the images in the MVTec AD dataset, i.e. single centered objects without much variation in background or images completely covered by textures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
