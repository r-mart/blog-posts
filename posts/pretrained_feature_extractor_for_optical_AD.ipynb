{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained feature extractor for optical anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "import timm\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "import bokeh\n",
    "\n",
    "import rootutils\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import save_plot_from_notbook_for_jekyll, bokeh_notebook_setup, save_plot_from_notebook_to_html\n",
    "from src.visualization.image import plot_img_rgba, add_bboxes_on_img, add_seg_on_img, plot_img_scalar, add_score_map_on_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/transistor\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- task similar to previous post\n",
    "  - we don't have regular patterns anymore\n",
    "  - we want to have a method that can be applied to several tasks\n",
    "- previous research resulted in high effectiveness for features extracted from Deep Learning models pre-trained on ImageNet\n",
    "  - reference to SPADE, Gaussian AD, PaDim and PatchCore\n",
    "- we will be following the approach of these papers but change a few components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Like in the previous post, we will use the [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad) which you can download from the website.\n",
    "The dataset contains 15 different categories. For the examples in this post we will use the 'Transistor' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a normal example without anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"train/good/000.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in contrast an anomalous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"test/bent_lead/000.png\"\n",
    "seg_path = data_path / \"ground_truth/bent_lead/000_mask.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.convert(\"RGBA\")\n",
    "\n",
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "p = plot_img_rgba(img)\n",
    "p = add_seg_on_img(p, seg)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "TODO: Update using native torchvision models with the new API (e.g. automatically load transforms): See [MODELS AND PRE-TRAINED WEIGHTS](https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "Like in the PaDim or PatchCore paper we are going to extract features for each image patch of the training set using a neural network architecture for vision tasks pre-trained on the ImageNet dataset. The patch size is determined by our choice for the network layer.\n",
    "To do the feature extraction we use the PyTorch `feature_extraction` package [based on Torch FX](https://pytorch.org/blog/FX-feature-extraction-torchvision/).\n",
    "The goal of this post is to demonstrate the principle rather than optimizing our approach to the dataset. Hence, we will simplify many steps compared to the paper.\n",
    "\n",
    "For the backbone we pick ResNet 34 as it is simple to use and doesn't require much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\"resnet34\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the papers features from several layers were combined. To keep it simple, we will use only one layer.\n",
    "To see the available layer names for feature extraction you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `train_nodes` or `eval_nodes`, you will see that ResNet 34 has 4 main blocks. If you just want to pick the last node of a block, the feature_extraction module allows you to use truncated node names. We will use `'layer2'` to get the last node of all the `layer2.x.ops` nodes. We choose layer 2 as a compromise between having expressive high-level features but still a somewhat high spatial feature map resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(backbone, return_nodes=[\"layer2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the paper, we fix the weights to the pre-trained ImageNet weights. Hence, we can turn off gradient computation to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify experimenting with different configurations, we use a Config object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    img_shape = (256, 256)  # height, width (multiple of 16)\n",
    "    batch_size = 4\n",
    "    num_workers: int = 2  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    red_factor = 8  # spatial reduction factor (depends on the chosen ResNet layer)\n",
    "    n_feats = 128  # number of features (depends on the chosen ResNet layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: shorten the explanation. I can't assume ppl have read the paper\n",
    "\n",
    "To save the features we will follow the memory bank approach from PatchCore. This means we will save the extracted features into a large array without linking them to the original patch location. This is in contrast to the PaDim approach. The advantage is that our approach becomes more robust to rotations, translations and other variations of the objects in the dataset. The disadvantage is that the number of feature vectors we have to compare each patch to becomes quite large. \n",
    "TODO brief calculation: N images * Height * Width of feature maps\n",
    "To use a method like nearest neighbor lookup like in the PatchCore paper, this requires tricks like the coreset reduction. \n",
    "We will get around this by choosing a different anomaly detection approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a pyTorch Dataset object to hold the image data and specify the necessary transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_path: os.PathLike, transforms: Optional[A.Compose] = None):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using a backbone network pre-trained on ImageNet, we need to apply the same normalization transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "\n",
    "default_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(Config.img_shape[0], Config.img_shape[1]),\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = TrainDataset(train_path, transforms=default_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we create the DataLoader object to feed our data to the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function containing the logic to call the feature extractor with a batch of images and collect the resulting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug (TODO: remove)\n",
    "# imgs = next(iter(train_dl))\n",
    "# print(feature_extractor(imgs)['layer2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature_dict = extractor(imgs)\n",
    "\n",
    "    layer = list(feature_dict.keys())[0]\n",
    "\n",
    "    feats = feature_dict[layer]\n",
    "\n",
    "    feats = feats.cpu().numpy()\n",
    "    feats = np.transpose(feats, (0, 2, 3, 1))\n",
    "    feats = feats.reshape(-1, cfg.n_feats)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put everything together to compute the feature memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = Config.img_shape[:2]\n",
    "\n",
    "# ResNet resolution reduction of target layer\n",
    "h_layer = math.ceil(h / Config.red_factor)\n",
    "w_layer = math.ceil(w / Config.red_factor)\n",
    "\n",
    "memory_bank_size = len(train_ds) * h_layer * w_layer\n",
    "memory_bank = np.empty((memory_bank_size, Config.n_feats), dtype=np.float32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_mem = 0\n",
    "\n",
    "for i, imgs in enumerate(train_dl):\n",
    "    n_samples = imgs.shape[0]\n",
    "\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    memory_bank[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the memory bank shape, we see that it contains over 200k feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in the next step in which we want to compute anomaly scores for each patch of a test image by comparing with the memory bank we need an anomaly detection approach which can deal with such a large number of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "For the anomaly detection part, we will extract the features of a target image with the same model as before. Afterwards, we will apply an off-the-shelf anomaly detection algorithm from the [Python Outlier Detection (PyOD) library](https://github.com/yzhao062/pyod).\n",
    "\n",
    "Side remark: I will use the terms anomaly detection and outlier detection interchangeably.\n",
    "\n",
    "TODO: why do we pick LUNAR\n",
    "\n",
    "Fitting the anomaly detection model on this large set of feature vectors may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LUNAR()\n",
    "clf.fit(memory_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we pick a defect image from the training data and extract its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_path / \"test/bent_lead/000.png\"\n",
    "seg_path = data_path / \"ground_truth/bent_lead/000_mask.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "img_np = np.array(img)\n",
    "img_t = default_transforms(image=img_np)[\"image\"]\n",
    "img_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "test_feats = get_features(img_t, feature_extractor, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an anomaly score map, we have to reshape the features to first match the image patch locations and eventually resize it to the original image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores = clf.decision_function(test_feats)\n",
    "score_patches = np.expand_dims(ano_scores, 0)\n",
    "score_patches = score_patches.reshape(h_layer, w_layer)\n",
    "\n",
    "anomaly_map = cv2.resize(score_patches, (img.width, img.height))\n",
    "\n",
    "# apply Gaussian blur to smooth out possible resizing artifacts\n",
    "anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "\n",
    "# make anomaly scores start at 0\n",
    "anomaly_map = anomaly_map - anomaly_map.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to overlay the anomaly score map with the original defect image and to compare with the ground truth annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "p_img = plot_img_rgba(img, title=\"Image with ground truth annotation\")\n",
    "p_img = add_seg_on_img(p_img, seg)\n",
    "p_ano = plot_img_rgba(img, title=\"Image with prediction\")\n",
    "p_ano = add_score_map_on_img(p_ano, anomaly_map, alpha=0.6)\n",
    "p = bokeh.layouts.row(p_img, p_ano)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we can see how the area with the highest anomaly scores correspond to the marked ground-truth defect annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    def __init__(self, transforms, feature_extractor, clf, cfg) -> None:\n",
    "        self.transforms = transforms\n",
    "        self.feature_extractor = feature_extractor.to(cfg.device)\n",
    "        self.clf = clf\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.h_layer = math.ceil(cfg.img_shape[0] / cfg.red_factor)\n",
    "        self.w_layer = math.ceil(cfg.img_shape[1] / cfg.red_factor)\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> np.ndarray:\n",
    "        img_np = np.array(img)\n",
    "        img_t = self.transforms(image=img_np)[\"image\"]\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "        feats = get_features(img_t, self.feature_extractor, self.cfg)\n",
    "\n",
    "        ano_scores = self.clf.decision_function(feats)\n",
    "        score_patches = np.expand_dims(ano_scores, 0)\n",
    "        score_patches = score_patches.reshape(self.h_layer, self.w_layer)\n",
    "\n",
    "        anomaly_map = cv2.resize(score_patches, (img.width, img.height))\n",
    "\n",
    "        # apply Gaussian blur to smooth out possible resizing artifacts\n",
    "        anomaly_map = gaussian_filter(anomaly_map, sigma=4)\n",
    "        anomaly_map = anomaly_map - anomaly_map.min()\n",
    "\n",
    "        return anomaly_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our new anomaly detector on different defect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AnomalyDetector(default_transforms, feature_extractor, clf, Config)\n",
    "\n",
    "img_it = (data_path / \"test/damaged_case\").iterdir()\n",
    "seg_it = (data_path / \"ground_truth/damaged_case\").iterdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = next(img_it)\n",
    "seg_path = next(seg_it)\n",
    "\n",
    "img = Image.open(img_path)\n",
    "seg = Image.open(seg_path)\n",
    "seg = np.array(seg)\n",
    "\n",
    "anomaly_map = detector(img)\n",
    "\n",
    "p_img = plot_img_rgba(img)\n",
    "p_img = add_seg_on_img(p_img, seg)\n",
    "p_ano = plot_img_rgba(img)\n",
    "p_ano = add_score_map_on_img(p_ano, anomaly_map, alpha=0.6)\n",
    "p = bokeh.layouts.row(p_img, p_ano)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "To quantify how well this approach works over all test data, we will make anomaly score predictions over all test images and compute the area under receiver operating characteristic curve (AUROC) metric. See [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for more details. This allows also to compare the approach with recent literature.\n",
    "\n",
    "Like in the 'training' phase, we first create a pyTorch Dataset. As the predictor class can handle already native python image objects, we don't necessarily need a DataLoader. The DataLoader would allow us to speed up the validation process by using batches but for this blog post we will keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "    ):\n",
    "        super(ValidationDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(gt_path / p.name / img_path.name)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if not gt_path.exists():\n",
    "            # there are no gt annotations for good cases -> all 0\n",
    "            gt = np.zeros_like(img)\n",
    "        else:\n",
    "            gt = Image.open(gt_path)\n",
    "            gt = gt.convert(\"L\")\n",
    "            gt = np.array(gt)\n",
    "            gt = gt / 255\n",
    "\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "val_ds = ValidationDataset(val_path, gt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can loop through the validation dataset and store ground truth and anomaly score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt = val_ds[0]\n",
    "\n",
    "pred_size = len(val_ds) * img.height * img.width\n",
    "preds_pix = np.empty(pred_size, dtype=np.float32)\n",
    "gts_pix = np.empty(pred_size, dtype=np.int32)\n",
    "preds_img = np.empty(len(val_ds), dtype=np.float32)\n",
    "gts_img = np.empty(len(val_ds), dtype=np.int32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_pix = 0\n",
    "\n",
    "for i in range(len(val_ds)):\n",
    "    img, gt = val_ds[i]\n",
    "    gt = gt.astype(np.int32)\n",
    "\n",
    "    anomaly_map = detector(img)\n",
    "    n_pix = anomaly_map.shape[0] * anomaly_map.shape[1]\n",
    "\n",
    "    preds_pix[i_pix : i_pix + n_pix] = anomaly_map.reshape((-1,))\n",
    "    gts_pix[i_pix : i_pix + n_pix] = gt.reshape((-1,))\n",
    "\n",
    "    # use max score of the map as image-level anomaly score\n",
    "    preds_img[i] = anomaly_map.max()\n",
    "    # for good images gt will be all zero, for defect images max will be 1\n",
    "    gts_img[i] = gt.max()\n",
    "\n",
    "    i_pix += n_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUROC score is computed using the ground truth values and prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_img, tpr_img, thresholds_img = roc_curve(gts_img, preds_img)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    title=f\"ROC curve for image-wise prediction (area = {auroc_img:.5f})\",\n",
    "    x_axis_label=\"False Positive Rate\",\n",
    "    y_axis_label=\"True Positive Rate\",\n",
    ")\n",
    "p.line(fpr_img, tpr_img, line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel-wise AUROC (TODO keep?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_pix, tpr_pix, thresholds_pix = roc_curve(gts_pix, preds_pix)\n",
    "# auroc_pix = auc(fpr_pix, tpr_pix)\n",
    "\n",
    "# print(f\"pixel-wise AUROC: {auroc_pix:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
