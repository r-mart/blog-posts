{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing features from pre-trained models for anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import umap\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import list_models, get_model\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import (\n",
    "    save_plot_from_notbook_for_jekyll,\n",
    "    bokeh_notebook_setup,\n",
    ")\n",
    "from src.visualization.image import (\n",
    "    plot_img_rgba,\n",
    "    add_seg_on_img,\n",
    "    add_score_map_on_img,\n",
    ")\n",
    "from src.visualization.features import plot_feature_samples, plot_feature_3d_samples, plot_labelled_feature_samples, plot_labelled_feature_3d_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()\n",
    "\n",
    "# make random number generator repeatable\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/wood\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- take feature extraction approach as in previous blog post\n",
    "- apply normal PCA to reduce to 2 dimensions\n",
    "- repeat with modified PCA to keep the feature combinations with smallest variance\n",
    "- plot and compar normal and anomalous features\n",
    "- repeat experiment with PCA reduction to 3 dimensions\n",
    "  - explore 3d plots with bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Use again 'Metal Nut' category from [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "See last post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"convnext_base\"\n",
    "    layer_names = [\"features.6\", \"features.7\"]\n",
    "    img_shape = (224, 224)  # height, width\n",
    "    batch_size = 16\n",
    "    num_workers: int = 8  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_feats = None # number of features (depends on the chosen layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.pool_layer = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_dict = self.feature_extractor(x)\n",
    "        for k, v in feature_dict.items():\n",
    "            feature_dict[k] = self.pool_layer(v)\n",
    "\n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = get_model(Config.model_name, weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nodes, eval_nodes = get_graph_node_names(backbone)\n",
    "# train_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(backbone, return_nodes=Config.layer_names)\n",
    "feature_extractor=GlobalFeatureExtractor(feature_extractor)\n",
    "\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature_dict = extractor(imgs)\n",
    "\n",
    "    layers = list(feature_dict.keys())\n",
    "    l_feats = [feature_dict[layer].squeeze((2, 3)) for layer in layers]\n",
    "    feats = torch.cat(l_feats, 1)\n",
    "    feats = feats.cpu().numpy()\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def get_ground_truths(labels, cfg):\n",
    "    labels = labels.numpy()\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "        N_train: Optional[int] = None,\n",
    "    ):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if N_train is not None and len(self.img_paths) > N_train:\n",
    "            self.img_paths = random.sample(self.img_paths, N_train)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "val_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "default_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(Config.img_shape[0], Config.img_shape[1]),\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = TrainDataset(train_path, transforms=default_transforms)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(train_dl))\n",
    "feats_shapes = []\n",
    "\n",
    "for layer_name in Config.layer_names:\n",
    "    feats_shapes.append(feature_extractor(imgs)[layer_name].shape)\n",
    "\n",
    "Config.n_feats = sum([fs[1] for fs in feats_shapes])\n",
    "print(\"n feats:\", Config.n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.empty((len(train_ds), Config.n_feats), dtype=np.float32)\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "i_mem = 0\n",
    "\n",
    "for i, imgs in enumerate(train_dl):\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    train_features[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train features shape:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "    ):\n",
    "        super(ValidationDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "        self.gt_class_name_to_label_map = {p.name: i for i, p in enumerate(gt_class_paths)}\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(\n",
    "                    gt_path / p.name / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    "                )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.gt_paths[index]\n",
    "\n",
    "        label = self.gt_class_name_to_label_map[gt_path.parent.name]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            img = np.array(img)\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ValidationDataset(val_path, gt_path, transforms=default_transforms)\n",
    "\n",
    "good_label = val_ds.gt_class_name_to_label_map['good']\n",
    "label_to_name_map = {v: k for k, v in val_ds.gt_class_name_to_label_map.items()}\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.empty((len(val_ds), Config.n_feats), dtype=np.float32)\n",
    "val_labels = np.zeros((len(val_ds)), dtype=np.uint32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "i_mem = 0\n",
    "\n",
    "for i, (imgs, labels) in enumerate(val_dl):\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    n_samples = feats.shape[0]\n",
    "    labels = get_ground_truths(labels, Config)\n",
    "\n",
    "    val_features[i_mem : i_mem + n_samples] = feats\n",
    "    val_labels[i_mem : i_mem + n_samples] = labels\n",
    "\n",
    "    i_mem += n_samples\n",
    "\n",
    "print(\"Validation features shape:\", val_features.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_gt = (val_labels != good_label).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Look at [Gaussian-AD code](https://github.com/ORippler/gaussian-ad-mvtec/blob/bc10bd736d85b750410e6b0e7ac843061e09511e/src/gaussian/model.py#L207) for PCA keeping features with least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "pca = PCA(n_components=None).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_features\n",
    "y = ano_gt\n",
    "y_label = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_thresholds = [0.9, 0.99]\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "X_pca = np.matmul(X_val, pca_comps.T)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[1] :]\n",
    "X_npca = np.matmul(X_val, npca_comps.T)\n",
    "\n",
    "print(X_pca.shape)\n",
    "print(X_npca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_dim = 2\n",
    "\n",
    "# umap_for_all = umap.UMAP(n_components=n_dim)\n",
    "# X_all_embed = umap_for_all.fit_transform(X_val)\n",
    "\n",
    "# umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "# X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "# umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "# X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_all = plot_feature_samples(\n",
    "#     X_all_embed, y, title=\"Feature embedding for all features\", width=400, height=400, alpha=1.0\n",
    "# )\n",
    "\n",
    "# p_pca = plot_feature_samples(\n",
    "#     X_pca_embed, y, title=\"Feature embedding after standard PCA\", width=400, height=400\n",
    "# )\n",
    "# p_npca = plot_feature_samples(\n",
    "#     X_npca_embed, y, title=\"Feature embedding after negative PCA\", width=400, height=400\n",
    "# )\n",
    "# p = bokeh.layouts.row(p_all, p_pca, p_npca)\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_all = plot_labelled_feature_samples(\n",
    "#     X_all_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for all features\", width=400, height=400\n",
    "# )\n",
    "\n",
    "# p_pca = plot_labelled_feature_samples(\n",
    "#     X_pca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after standard PCA\", width=400, height=400\n",
    "# )\n",
    "# p_npca = plot_labelled_feature_samples(\n",
    "#     X_npca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after negative PCA\", width=400, height=400\n",
    "# )\n",
    "# p = bokeh.layouts.row(p_all, p_pca, p_npca)\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_npca = plot_labelled_feature_samples(\n",
    "#     X_npca_embed,\n",
    "#     y_label,\n",
    "#     label_to_name_map=label_to_name_map,\n",
    "#     title=\"Feature embedding after negative PCA\",\n",
    "#     width=800,\n",
    "#     height=800,\n",
    "#     alpha=1.0,\n",
    "# )\n",
    "# show(p_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 3\n",
    "\n",
    "umap_for_all = umap.UMAP(n_components=n_dim)\n",
    "X_all_embed = umap_for_all.fit_transform(X_val)\n",
    "\n",
    "umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_all_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for all features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_pca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after standard PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_npca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after negative PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(\n",
    "    values: np.ndarray, mean: np.ndarray, inv_covariance: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the batched mahalanobis distance.\n",
    "    values is a batch of feature vectors.\n",
    "    mean is either the mean of the distribution to compare, or a second\n",
    "    batch of feature vectors.\n",
    "    inv_covariance is the inverse covariance of the target distribution.\n",
    "    \"\"\"\n",
    "    assert values.ndim == 2\n",
    "    assert 1 <= mean.ndim <= 2\n",
    "    assert len(inv_covariance.shape) == 2\n",
    "    assert values.shape[1] == mean.shape[-1]\n",
    "    assert mean.shape[-1] == inv_covariance.shape[0]\n",
    "    assert inv_covariance.shape[0] == inv_covariance.shape[1]\n",
    "\n",
    "    if mean.ndim == 1:  # Distribution mean.\n",
    "        mean = np.expand_dims(mean, 0)\n",
    "    x_mu = values - mean  # batch x features\n",
    "    # Same as dist = x_mu.t() * inv_covariance * x_mu batch wise\n",
    "    dist = np.einsum(\"im,mn,in->i\", x_mu, inv_covariance, x_mu)\n",
    "    return np.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_thresholds = [0.99]\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "\n",
    "train_features_pca = np.matmul(train_features, pca_comps.T)\n",
    "val_features_pca = np.matmul(val_features, pca_comps.T)\n",
    "\n",
    "print(\"PCA training features shape\", train_features_pca.shape)\n",
    "print(\"PCA validation features shape\", val_features_pca.shape)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[0]:]\n",
    "\n",
    "train_features_npca = np.matmul(train_features, npca_comps.T)\n",
    "val_features_npca = np.matmul(val_features, npca_comps.T)\n",
    "\n",
    "print(\"NPCA training features shape\", train_features_npca.shape)\n",
    "print(\"NPCA validation features shape\", val_features_npca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LUNAR(n_neighbours=5)\n",
    "# clf = IForest()\n",
    "# clf = KNN(n_neighbors=5)\n",
    "# clf.fit(train_features)\n",
    "\n",
    "# model_path = output_path / 'clf.pkl'\n",
    "# pickle.dump(clf, open(model_path, 'wb'))\n",
    "# clf = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_npca = np.mean(train_features_npca, axis=0)\n",
    "lw_cov_npca = LedoitWolf().fit(train_features_npca)\n",
    "inv_cov_npca = lw_cov_npca.precision_ \n",
    "\n",
    "ano_scores_npca = mahalanobis_distance(val_features_npca, mean_npca, inv_cov_npca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_npca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"NPCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pca = np.mean(train_features_pca, axis=0)\n",
    "lw_cov_pca = LedoitWolf().fit(train_features_pca)\n",
    "inv_cov_pca = lw_cov_pca.precision_ \n",
    "\n",
    "ano_scores_pca = mahalanobis_distance(val_features_pca, mean_pca, inv_cov_pca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_pca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"PCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_path = output_path / \"ROC_curve.html\"\n",
    "# save_plot_from_notbook_for_jekyll(p, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
