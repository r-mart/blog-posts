{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing features for anomaly detection from pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import umap\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import list_models, get_model\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "import bokeh\n",
    "\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import (\n",
    "    save_plot_from_notbook_for_jekyll,\n",
    "    bokeh_notebook_setup,\n",
    ")\n",
    "from src.visualization.image import (\n",
    "    plot_img_rgba,\n",
    "    add_seg_on_img,\n",
    "    add_score_map_on_img,\n",
    ")\n",
    "from src.visualization.features import plot_feature_samples, plot_feature_3d_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()\n",
    "\n",
    "# make random number generator repeatable\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/metal_nut\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- take feature extraction approach as in previous blog post\n",
    "- apply normal PCA to reduce to 2 dimensions\n",
    "- repeat with modified PCA to keep the feature combinations with smallest variance\n",
    "- plot and compar normal and anomalous features\n",
    "- repeat experiment with PCA reduction to 3 dimensions\n",
    "  - explore 3d plots with bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Use again 'Metal Nut' category from [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "See last post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"convnext_base\"\n",
    "    layer_names = [\"features.3\"]\n",
    "    img_shape = (224, 224)  # height, width\n",
    "    batch_size = 4\n",
    "    num_workers: int = 2  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    red_factor = None  # spatial reduction factor (equivalent to patch size)\n",
    "    n_feats = None  # number of features (depends on the chosen layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCoreModel(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.patch_layer = torch.nn.AvgPool2d(3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_dict = self.feature_extractor(x)\n",
    "        for k, v in feature_dict.items():\n",
    "            feature_dict[k] = self.patch_layer(v)\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "        N_train: Optional[int] = None,\n",
    "    ):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if N_train is not None and len(self.img_paths) > N_train:\n",
    "            self.img_paths = random.sample(self.img_paths, N_train)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "    ):\n",
    "        super(ValidationDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(\n",
    "                    gt_path / p.name / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    "                )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.gt_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if not gt_path.exists():\n",
    "            # there are no gt annotations for good cases -> all 0\n",
    "            gt = np.zeros((img.height, img.width))\n",
    "        else:\n",
    "            gt = Image.open(gt_path)\n",
    "            gt = gt.convert(\"L\")\n",
    "            gt = np.array(gt)\n",
    "            gt = gt / 255\n",
    "\n",
    "        if self.transforms:\n",
    "            img = np.array(img)\n",
    "            transformed = self.transforms(image=img, mask=gt)\n",
    "            img = transformed[\"image\"]\n",
    "            gt = transformed[\"mask\"]\n",
    "\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature_dict = extractor(imgs)\n",
    "\n",
    "    layers = list(feature_dict.keys())\n",
    "\n",
    "    feats = feature_dict[layers[0]]\n",
    "    feats = feats.cpu().numpy()\n",
    "    feats = np.transpose(feats, (0, 2, 3, 1))\n",
    "    feats = feats.reshape(-1, cfg.n_feats)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def get_ground_truths(gts, cfg):\n",
    "    gts = gts.numpy()\n",
    "    feat_shape = [s // cfg.red_factor for s in gts[0].shape[:2]]\n",
    "    patch_gts = np.zeros((gts.shape[0], *feat_shape))\n",
    "\n",
    "    for i, gt in enumerate(gts):\n",
    "        gt = cv2.resize(gt, dsize=feat_shape, interpolation=cv2.INTER_LINEAR)\n",
    "        # gt = np.rint(gt)\n",
    "        gt = np.floor(gt)\n",
    "        patch_gts[i] = gt\n",
    "\n",
    "    patch_gts = patch_gts.reshape(-1)\n",
    "\n",
    "    return patch_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = get_model(Config.model_name, weights=\"DEFAULT\")\n",
    "feature_extractor = create_feature_extractor(backbone, return_nodes=Config.layer_names)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor = PatchCoreModel(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "val_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "default_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(Config.img_shape[0], Config.img_shape[1]),\n",
    "        A.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = TrainDataset(train_path, transforms=default_transforms)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(train_dl))\n",
    "feats_shapes = []\n",
    "\n",
    "for layer_name in Config.layer_names:\n",
    "    feats_shapes.append(feature_extractor(imgs)[layer_name].shape)\n",
    "\n",
    "Config.n_feats = sum([fs[1] for fs in feats_shapes])\n",
    "Config.red_factor = Config.img_shape[0] // feats_shapes[0][2]\n",
    "\n",
    "print(\"n feats:\", Config.n_feats)\n",
    "print(\"red factor:\", Config.red_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = Config.img_shape[:2]\n",
    "h_layer = math.ceil(h / Config.red_factor)\n",
    "w_layer = math.ceil(w / Config.red_factor)\n",
    "\n",
    "memory_bank_size = len(train_ds) * h_layer * w_layer\n",
    "memory_bank = np.empty((memory_bank_size, Config.n_feats), dtype=np.float32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_mem = 0\n",
    "\n",
    "for i, imgs in enumerate(train_dl):\n",
    "    n_samples = imgs.shape[0]\n",
    "\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    memory_bank[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train memory bank shape:\", memory_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ValidationDataset(val_path, gt_path, transforms=default_transforms)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_feat_bank_size = len(val_ds) * h_layer * w_layer\n",
    "val_feat_bank = np.empty((val_feat_bank_size, Config.n_feats), dtype=np.float32)\n",
    "val_gt_bank = np.zeros((val_feat_bank_size), dtype=np.float32)\n",
    "\n",
    "feature_extractor = feature_extractor.to(Config.device)\n",
    "\n",
    "i_mem = 0\n",
    "\n",
    "for i, (imgs, gts) in enumerate(val_dl):\n",
    "    n_samples = imgs.shape[0]\n",
    "\n",
    "    feats = get_features(imgs, feature_extractor, Config)\n",
    "    patch_gts = get_ground_truths(gts, Config)\n",
    "    val_feat_bank[i_mem : i_mem + feats.shape[0]] = feats\n",
    "    val_gt_bank[i_mem : i_mem + patch_gts.shape[0]] = patch_gts\n",
    "\n",
    "    i_mem += feats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation feature bank shape:\", val_feat_bank.shape)\n",
    "print(\"Validation ground truth bank shape:\", val_gt_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Look at [Gaussian-AD code](https://github.com/ORippler/gaussian-ad-mvtec/blob/bc10bd736d85b750410e6b0e7ac843061e09511e/src/gaussian/model.py#L207) for PCA keeping features with least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = memory_bank\n",
    "pca = PCA(n_components=None).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_feat_bank[::50]\n",
    "y = val_gt_bank[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold = 0.9\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "i_comp_thresh = (variances > variance_threshold).argmax()\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresh + 1]\n",
    "X_pca = np.matmul(X_val, pca_comps.T)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresh - 1 :]\n",
    "X_npca = np.matmul(X_val, npca_comps.T)\n",
    "\n",
    "print(X_pca.shape)\n",
    "print(X_npca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2\n",
    "\n",
    "umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "X_npca_embed = umap_for_npca.fit_transform(X_npca)\n",
    "\n",
    "p_pca = plot_feature_samples(\n",
    "    X_pca_embed, y, title=\"Feature embedding after standard PCA\", width=600, height=600\n",
    ")\n",
    "p_npca = plot_feature_samples(\n",
    "    X_npca_embed, y, title=\"Feature embedding after negative PCA\", width=600, height=600\n",
    ")\n",
    "p = bokeh.layouts.row(p_pca, p_npca)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 3\n",
    "\n",
    "umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_feature_3d_samples(X_pca_embed, y, title=\"Feature embedding after standard PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_feature_3d_samples(X_npca_embed, y, title=\"Feature embedding after negative PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_path = output_path / \"ROC_curve.html\"\n",
    "# save_plot_from_notbook_for_jekyll(p, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
