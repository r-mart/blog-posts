{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing features from pre-trained DINOv2 model for anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import umap\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import (\n",
    "    save_plot_from_notbook_for_jekyll,\n",
    "    bokeh_notebook_setup,\n",
    ")\n",
    "from src.visualization.image import (\n",
    "    plot_img_rgba,\n",
    "    add_seg_on_img,\n",
    "    add_score_map_on_img,\n",
    ")\n",
    "from src.visualization.features import plot_feature_samples, plot_feature_3d_samples, plot_labelled_feature_samples, plot_labelled_feature_3d_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2ForImageClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()\n",
    "\n",
    "# make random number generator repeatable\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/wood\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- take feature extraction approach as in previous blog post\n",
    "- apply normal PCA to reduce to 2 dimensions\n",
    "- repeat with modified PCA to keep the feature combinations with smallest variance\n",
    "- plot and compar normal and anomalous features\n",
    "- repeat experiment with PCA reduction to 3 dimensions\n",
    "  - explore 3d plots with bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Use a category from [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "See last post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 16\n",
    "    num_workers: int = 8  # adjust to the number of processing cores you want to use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_feats = None # number of features (depends on the chosen layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try the non-classification[DINOv2 MOdel](https://huggingface.co/docs/transformers/main/model_doc/dinov2#transformers.Dinov2Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dinov2ForImageClassification.from_pretrained(\"facebook/dinov2-small-imagenet1k-1-layer\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove classification layer\n",
    "# model.classifier = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small-imagenet1k-1-layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = extractor(**imgs).logits\n",
    "\n",
    "    feats = feats.cpu().numpy()\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        processor: Optional[A.Compose] = None,\n",
    "        N_train: Optional[int] = None,\n",
    "    ):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.processor = processor\n",
    "\n",
    "        if N_train is not None and len(self.img_paths) > N_train:\n",
    "            self.img_paths = random.sample(self.img_paths, N_train)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.processor:\n",
    "            img = self.processor(img, return_tensors=\"pt\")\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "val_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "train_ds = TrainDataset(train_path, processor=image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = train_ds[0]\n",
    "feats_shapes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = model(**imgs).logits\n",
    "\n",
    "Config.n_feats = feats.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.empty((len(train_ds), Config.n_feats), dtype=np.float32)\n",
    "model = model.to(Config.device)\n",
    "i_mem = 0\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    imgs = train_ds[i]\n",
    "    \n",
    "    feats = get_features(imgs, model, Config)\n",
    "    train_features[i] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train features shape:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "        processor: Optional[A.Compose] = None,\n",
    "    ):\n",
    "        super(ValidationDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "        self.gt_class_name_to_label_map = {p.name: i for i, p in enumerate(gt_class_paths)}\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(\n",
    "                    gt_path / p.name / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    "                )\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.gt_paths[index]\n",
    "\n",
    "        label = self.gt_class_name_to_label_map[gt_path.parent.name]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.processor:\n",
    "            img = self.processor(img, return_tensors=\"pt\")\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ValidationDataset(val_path, gt_path, processor=image_processor)\n",
    "\n",
    "good_label = val_ds.gt_class_name_to_label_map['good']\n",
    "label_to_name_map = {v: k for k, v in val_ds.gt_class_name_to_label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.empty((len(val_ds), Config.n_feats), dtype=np.float32)\n",
    "val_labels = np.zeros((len(val_ds)), dtype=np.uint32)\n",
    "\n",
    "model = model.to(Config.device)\n",
    "i_mem = 0\n",
    "\n",
    "for i in range(len(val_ds)):\n",
    "    imgs, label = val_ds[i]\n",
    "\n",
    "    feats = get_features(imgs, model, Config)\n",
    "\n",
    "    val_features[i] = feats\n",
    "    val_labels[i] = label\n",
    "\n",
    "print(\"Validation features shape:\", val_features.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_gt = (val_labels != good_label).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Look at [Gaussian-AD code](https://github.com/ORippler/gaussian-ad-mvtec/blob/bc10bd736d85b750410e6b0e7ac843061e09511e/src/gaussian/model.py#L207) for PCA keeping features with least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "pca = PCA(n_components=None).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_features\n",
    "y = ano_gt\n",
    "y_label = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_thresholds = [0.9, 0.99]\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "X_pca = np.matmul(X_val, pca_comps.T)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[1] :]\n",
    "X_npca = np.matmul(X_val, npca_comps.T)\n",
    "\n",
    "print(X_pca.shape)\n",
    "print(X_npca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_dim = 2\n",
    "\n",
    "# umap_for_all = umap.UMAP(n_components=n_dim)\n",
    "# X_all_embed = umap_for_all.fit_transform(X_val)\n",
    "\n",
    "# umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "# X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "# umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "# X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_all = plot_feature_samples(\n",
    "#     X_all_embed, y, title=\"Feature embedding for all features\", width=400, height=400, alpha=1.0\n",
    "# )\n",
    "\n",
    "# p_pca = plot_feature_samples(\n",
    "#     X_pca_embed, y, title=\"Feature embedding after standard PCA\", width=400, height=400\n",
    "# )\n",
    "# p_npca = plot_feature_samples(\n",
    "#     X_npca_embed, y, title=\"Feature embedding after negative PCA\", width=400, height=400\n",
    "# )\n",
    "# p = bokeh.layouts.row(p_all, p_pca, p_npca)\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_all = plot_labelled_feature_samples(\n",
    "#     X_all_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for all features\", width=400, height=400\n",
    "# )\n",
    "\n",
    "# p_pca = plot_labelled_feature_samples(\n",
    "#     X_pca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after standard PCA\", width=400, height=400\n",
    "# )\n",
    "# p_npca = plot_labelled_feature_samples(\n",
    "#     X_npca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after negative PCA\", width=400, height=400\n",
    "# )\n",
    "# p = bokeh.layouts.row(p_all, p_pca, p_npca)\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_npca = plot_labelled_feature_samples(\n",
    "#     X_npca_embed,\n",
    "#     y_label,\n",
    "#     label_to_name_map=label_to_name_map,\n",
    "#     title=\"Feature embedding after negative PCA\",\n",
    "#     width=800,\n",
    "#     height=800,\n",
    "#     alpha=1.0,\n",
    "# )\n",
    "# show(p_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 3\n",
    "\n",
    "umap_for_all = umap.UMAP(n_components=n_dim)\n",
    "X_all_embed = umap_for_all.fit_transform(X_val)\n",
    "\n",
    "umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_all_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for all features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_pca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after standard PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_npca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding after negative PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(\n",
    "    values: np.ndarray, mean: np.ndarray, inv_covariance: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the batched mahalanobis distance.\n",
    "    values is a batch of feature vectors.\n",
    "    mean is either the mean of the distribution to compare, or a second\n",
    "    batch of feature vectors.\n",
    "    inv_covariance is the inverse covariance of the target distribution.\n",
    "    \"\"\"\n",
    "    assert values.ndim == 2\n",
    "    assert 1 <= mean.ndim <= 2\n",
    "    assert len(inv_covariance.shape) == 2\n",
    "    assert values.shape[1] == mean.shape[-1]\n",
    "    assert mean.shape[-1] == inv_covariance.shape[0]\n",
    "    assert inv_covariance.shape[0] == inv_covariance.shape[1]\n",
    "\n",
    "    if mean.ndim == 1:  # Distribution mean.\n",
    "        mean = np.expand_dims(mean, 0)\n",
    "    x_mu = values - mean  # batch x features\n",
    "    # Same as dist = x_mu.t() * inv_covariance * x_mu batch wise\n",
    "    dist = np.einsum(\"im,mn,in->i\", x_mu, inv_covariance, x_mu)\n",
    "    return np.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_thresholds = [0.99]\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "\n",
    "train_features_pca = np.matmul(train_features, pca_comps.T)\n",
    "val_features_pca = np.matmul(val_features, pca_comps.T)\n",
    "\n",
    "print(\"PCA training features shape\", train_features_pca.shape)\n",
    "print(\"PCA validation features shape\", val_features_pca.shape)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[0]:]\n",
    "\n",
    "train_features_npca = np.matmul(train_features, npca_comps.T)\n",
    "val_features_npca = np.matmul(val_features, npca_comps.T)\n",
    "\n",
    "print(\"NPCA training features shape\", train_features_npca.shape)\n",
    "print(\"NPCA validation features shape\", val_features_npca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LUNAR(n_neighbours=5)\n",
    "# clf = IForest()\n",
    "# clf = KNN(n_neighbors=5)\n",
    "# clf.fit(train_features)\n",
    "\n",
    "# model_path = output_path / 'clf.pkl'\n",
    "# pickle.dump(clf, open(model_path, 'wb'))\n",
    "# clf = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_npca = np.mean(train_features_npca, axis=0)\n",
    "lw_cov_npca = LedoitWolf().fit(train_features_npca)\n",
    "inv_cov_npca = lw_cov_npca.precision_ \n",
    "\n",
    "ano_scores_npca = mahalanobis_distance(val_features_npca, mean_npca, inv_cov_npca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_npca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"NPCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pca = np.mean(train_features_pca, axis=0)\n",
    "lw_cov_pca = LedoitWolf().fit(train_features_pca)\n",
    "inv_cov_pca = lw_cov_pca.precision_ \n",
    "\n",
    "ano_scores_pca = mahalanobis_distance(val_features_pca, mean_pca, inv_cov_pca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_pca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"PCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_path = output_path / \"ROC_curve.html\"\n",
    "# save_plot_from_notbook_for_jekyll(p, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
