{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing features from pre-trained DINOv2 model for anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To autoreload external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import show\n",
    "import bokeh\n",
    "\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(Path.cwd(), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "from src.visualization.utils import (\n",
    "    save_plot_from_notbook_for_jekyll,\n",
    "    bokeh_notebook_setup,\n",
    ")\n",
    "from src.visualization.image import plot_img_rgba\n",
    "from src.visualization.features import plot_labelled_feature_3d_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh_notebook_setup()\n",
    "\n",
    "# make random number generator repeatable\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post we will classify images into normal or anomalous images. This is a common task in automatic optical inspection in the manufacturing industry. \n",
    "We consider the scenario where plenty of example images without any anomalies (normal samples) can be provided but example images with defects (anomalies) are rare.\n",
    "\n",
    "We therefore address this scenario with an anomaly detection approach that learns the distribution of normal samples during training. During inference it then scores target images based on how well they fit to the learned distribution. This approach gives one score for the whole image instead of determining pixel-wise anomaly maps (see the [previous blog post](https://r-mart.github.io/posts/pretrained-feature-extractor-for-optical-ad/) for the latter).\n",
    "\n",
    "Previous research has demonstrated a high effectiveness of features from Deep Learning models pre-trained on the ImageNet dataset. See for example the SOTA approaches on the [Anomaly Detection on MVTec AD benchmark](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad). This post is in particular inspired by the paper [Gaussian-AD - Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection](https://paperswithcode.com/paper/modeling-the-distribution-of-normal-data-in) which will be referred to as the 'Gaussian AD paper'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Like in the previous posts, we use the [MVTec anomaly detection dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad) which can be downloaded from the website.\n",
    "This time we consider the 'wood' category. The following figure shows example images from the test set for each of the 6 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/raw/wood\")\n",
    "output_path = Path(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = data_path / \"test\"\n",
    "p_list = []\n",
    "\n",
    "for class_path in test_path.iterdir():\n",
    "    label = class_path.name\n",
    "    img_path = class_path / \"000.png\"\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    p_list.append(plot_img_rgba(img, title=label, red_factor=4))\n",
    "\n",
    "\n",
    "n_cols = len(p_list) // 2\n",
    "\n",
    "p = bokeh.layouts.column(\n",
    "        bokeh.layouts.row(*p_list[:n_cols]),\n",
    "        bokeh.layouts.row(*p_list[n_cols:])\n",
    ")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'good' class has no defects. The other classes each show a different type of anomaly. The training data only contains images of the 'good' class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this approach we will extract one feature vector for each image in the training and test data. First, we define a class to hold the configs which consists of quite few options this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = 'facebook/dinov2-small'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_feats = None # number of features (depends on the chosen layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original Gaussian AD paper, features from individual layers have been extracted and combined. Since the release of the paper, however, a couple of powerful feature extraction architectures have been published.\n",
    "We are going to use the [Hugging Face implementation of the DINOv2 model](https://huggingface.co/docs/transformers/main/model_doc/dinov2) as it claims to be a stable 'all-purpose' feature extractor.\n",
    "\n",
    "We load a pretrained model from the `Dinov2Model` class from Hugging Face. See [Hugging Face models](https://huggingface.co/models?sort=created&search=facebook%2Fdinov2) for a list of available models.\n",
    "The `AutoImageProcessor` class loads the corresponding image pre-processing steps for the given model name. As we are not going to finetune any weights, we turn of gradients for the model parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dinov2Model.from_pretrained(Config.model_name)\n",
    "image_processor = AutoImageProcessor.from_pretrained(Config.model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feature vector for each image from this model, we simply access the output after the last pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(imgs, extractor, cfg):\n",
    "    imgs = imgs.to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = extractor(**imgs).pooler_output\n",
    "\n",
    "    feats = feats.cpu().numpy()\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "In the pyTorch `Dataset` class for loading the data we just load the images and apply the Hugging Face image processor corresponding to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        processor = None,\n",
    "    ):\n",
    "        super(TrainDataset).__init__()\n",
    "\n",
    "        self.img_paths = list(data_path.iterdir())\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.processor:\n",
    "            img = self.processor(img, return_tensors=\"pt\")\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path / \"train/good\"\n",
    "train_ds = TrainDataset(train_path, processor=image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sanity check the loading code and also determine the size of the extracted feature vector we run it for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(Config.device)\n",
    "imgs = train_ds[0]\n",
    "\n",
    "feats = get_features(imgs, model, Config)\n",
    "Config.n_feats = feats.shape[1]\n",
    "\n",
    "print(\"Feature shape:\", feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract and store all features for the training data by feeding the images to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.empty((len(train_ds), Config.n_feats), dtype=np.float32)\n",
    "model = model.to(Config.device)\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    imgs = train_ds[i]\n",
    "    feats = get_features(imgs, model, Config)\n",
    "    train_features[i] = feats\n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for a larger dataset I would recommend to use a pytorch 'DataLoader' class to use batch processing to speed up the feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "We also extract and store the features for the test data. Compared to the training data, the `Dataset` class needs more logic to find the various defect classes in the subfolders and return a label representing the defect class for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: os.PathLike,\n",
    "        gt_path: os.PathLike,\n",
    "        processor=None,\n",
    "    ):\n",
    "        super(TestDataset).__init__()\n",
    "\n",
    "        self.img_paths = list()\n",
    "        self.gt_paths = list()\n",
    "\n",
    "        gt_class_paths = list(data_path.iterdir())\n",
    "        self.gt_class_name_to_label_map = {\n",
    "            p.name: i for i, p in enumerate(gt_class_paths)\n",
    "        }\n",
    "\n",
    "        for p in gt_class_paths:\n",
    "            for img_path in p.iterdir():\n",
    "                self.img_paths.append(img_path)\n",
    "                self.gt_paths.append(\n",
    "                    gt_path / p.name / f\"{img_path.stem}_mask{img_path.suffix}\"\n",
    "                )\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.img_paths[index]\n",
    "        gt_path = self.gt_paths[index]\n",
    "\n",
    "        label = self.gt_class_name_to_label_map[gt_path.parent.name]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.processor:\n",
    "            img = self.processor(img, return_tensors=\"pt\")\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = data_path / \"test\"\n",
    "gt_path = data_path / \"ground_truth\"\n",
    "\n",
    "test_ds = TestDataset(test_path, gt_path, processor=image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we save the label for the 'good' class and a mapping from label to the class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_label = test_ds.gt_class_name_to_label_map['good']\n",
    "label_to_name_map = {v: k for k, v in test_ds.gt_class_name_to_label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the test dataset class defined, we can now extract the features and corresponding label for each image in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.empty((len(test_ds), Config.n_feats), dtype=np.float32)\n",
    "test_labels = np.zeros((len(test_ds)), dtype=np.uint32)\n",
    "\n",
    "model = model.to(Config.device)\n",
    "\n",
    "for i in range(len(test_ds)):\n",
    "    imgs, label = test_ds[i]\n",
    "\n",
    "    feats = get_features(imgs, model, Config)\n",
    "\n",
    "    test_features[i] = feats\n",
    "    test_labels[i] = label\n",
    "\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels (class names) will be mostly relevant for visualization. As our goal is to distinguish normal from anomalous images, we save the 'ground truth' values simply as '0' for normal (good) image and '1' for anomaly (any other class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_gt = (test_labels != good_label).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "After having the features for all images and the corresponding labels for the test images stored, we can get to the actual anomaly detection. We follow the Gaussian AD paper in fitting a multivariate Gaussian distribution to the extracted feature vectors of the training data. A multivariate Gaussian is parameterized by a mean vector and a covariance matrix. We therefore have to fit both to the training data. While the mean vector is simple, for the covariance matrix we use the [Ledoit Wolf Estimator](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html).\n",
    "\n",
    "We use the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) from the Gaussian distribution as anomaly score. It is basically the distance from the mean of the Gaussian which also takes the different variances and covariances for each direction in feature space into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianAD:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.lw_cov = None\n",
    "        self.lw_prec = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        lw_cov = LedoitWolf().fit(X)\n",
    "\n",
    "        self.lw_cov = lw_cov\n",
    "        self.lw_prec = lw_cov.precision_\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return mahalanobis_distance(X, self.mean, self.lw_prec)\n",
    "\n",
    "\n",
    "def mahalanobis_distance(\n",
    "    values: np.ndarray, mean: np.ndarray, inv_covariance: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the batched mahalanobis distance.\n",
    "    values is a batch of feature vectors.\n",
    "    mean is either the mean of the distribution to compare, or a second\n",
    "    batch of feature vectors.\n",
    "    inv_covariance is the inverse covariance of the target distribution.\n",
    "    \"\"\"\n",
    "    assert values.ndim == 2\n",
    "    assert 1 <= mean.ndim <= 2\n",
    "    assert len(inv_covariance.shape) == 2\n",
    "    assert values.shape[1] == mean.shape[-1]\n",
    "    assert mean.shape[-1] == inv_covariance.shape[0]\n",
    "    assert inv_covariance.shape[0] == inv_covariance.shape[1]\n",
    "\n",
    "    if mean.ndim == 1:  # Distribution mean.\n",
    "        mean = np.expand_dims(mean, 0)\n",
    "    x_mu = values - mean  # batch x features\n",
    "    # Same as dist = x_mu.t() * inv_covariance * x_mu batch wise\n",
    "    dist = np.einsum(\"im,mn,in->i\", x_mu, inv_covariance, x_mu)\n",
    "    return np.sqrt(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we fit the multivariate Gaussian to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianAD()\n",
    "clf.fit(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and consequently score the test data using the Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores = clf.decision_function(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the performance of this approach over the whole test dataset, we use the area under [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve (AUROC) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"Image-wise Anomaly Detection AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already gives a decent score compared to the [Anomaly Detection on MVTec AD benchmark](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad). (The comparison is not completely fair, as the benchmark score consists of the average for all MVTec AD categories, but it can still serve as an orientation)\n",
    "\n",
    "The Gaussian AD paper went one step further and analyzed the extracted features to get an intuition why they work so well. Inspired by the paper, we will look at the [principal components](https://en.wikipedia.org/wiki/Principal_component_analysis) of the training feature vectors in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "We start by fitting a PCA model to the training features to find how much variance is 'explained' by the principal components (feature eigen vectors corresponding to largest variance).<br>\n",
    "In particular, we save the indices for the principal components up until 90% and 99% of the variance of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "pca = PCA(n_components=None).fit(X_train)\n",
    "\n",
    "variance_thresholds = [0.9, 0.99]\n",
    "variances = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())\n",
    "\n",
    "print(\"Dimension of feature space:\", X_train.shape[1])\n",
    "\n",
    "for i in range(len(variance_thresholds)):\n",
    "    print(f\"The first {i_comp_thresholds[i]} features explain {variance_thresholds[i]*100}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this information to perform a standard PCA dimensionality reduction in which we keep the components explaining most of the variance. <br>\n",
    "However, we will also consider a 'negative PCA' dimensionality reduction in which we keep the components explaining the least variance. Note that we perform this reduction on the test data (with defect images) while the PCA components have been fitted to the training data (with good images only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_features\n",
    "y = ano_gt\n",
    "y_label = test_labels\n",
    "\n",
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "X_pca = np.matmul(X_test, pca_comps.T)\n",
    "\n",
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[1] :]\n",
    "X_npca = np.matmul(X_test, npca_comps.T)\n",
    "\n",
    "print(\"Test data shape after reduction with standard PCA:\", X_pca.shape)\n",
    "print(\"Test data shape after reduction with negative PCA:\", X_npca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an intuition how these lower dimensional feature vectors behave we visualize them by mapping onto the 3 dimensional space using [Uniform Manifold Approximation and Projection (UMAP)](https://umap-learn.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 3\n",
    "\n",
    "umap_for_all = umap.UMAP(n_components=n_dim)\n",
    "X_all_embed = umap_for_all.fit_transform(X_test)\n",
    "\n",
    "umap_for_pca = umap.UMAP(n_components=n_dim)\n",
    "X_pca_embed = umap_for_pca.fit_transform(X_pca)\n",
    "\n",
    "umap_for_npca = umap.UMAP(n_components=n_dim)\n",
    "X_npca_embed = umap_for_npca.fit_transform(X_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_all_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for the test samples using all features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP embedding for the complete feature vectors shows that there is a tendency for the different image classes to be clustered. This explains why the anomaly detection before was quite good.\n",
    "\n",
    "One has to be cautious though to interpret the result of such low-dimensional embeddings. The original feature space is 384 dimensional in our case. It is well possible that the features are separated in such high dimensional spaces while a 2D or 3D embedding shows them as mixed up. In general, the rule of thumb is\n",
    "$$ \\textrm{well separated cluster in embedding} \\Rightarrow \\textrm{well separated cluster in high-dim space} $$\n",
    "$$ \\textrm{mixed samples in embedding} \\nRightarrow  \\textrm{mixed samples in high-dim space} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_pca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for the test samples after standard PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the embeddings for the features that have been reduced using standard PCA (high variance components), we see that there is still a separation but the clusters are less clear. Note in particular how wide the 'good' samples are separated. This is what this feature selection is optimized for. However, in between the other samples are partly mixed in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_labelled_feature_3d_samples(X_npca_embed, y_label, label_to_name_map=label_to_name_map, title=\"Feature embedding for the test samples after negative PCA\")\n",
    "plt.show()\n",
    "#savefig(output_path/'npca_features_embedding.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that have been reduced using 'negative PCA' (low variance components) are in general less well separated (as expected). Compared to the previous plots, in particular the good samples form a tighter cluster now. \n",
    "Nevertheless, it seems to do an astonishingly good job in clustering the different classes which should also lead to a good anomaly detection performance.\n",
    "\n",
    "To confirm that, we will again determine the anomaly detection performance using the AUROC metric, but this time using only the high variance PCA components (up until 99% of variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_thresholds = [0.99]\n",
    "i_comp_thresholds = []\n",
    "for variance_threshold in variance_thresholds:\n",
    "    i_comp_thresholds.append((variances > variance_threshold).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal PCA\n",
    "pca_comps = pca.components_[: i_comp_thresholds[0]]\n",
    "\n",
    "train_features_pca = np.matmul(train_features, pca_comps.T)\n",
    "test_features_pca = np.matmul(test_features, pca_comps.T)\n",
    "\n",
    "print(\"PCA training features shape\", train_features_pca.shape)\n",
    "print(\"PCA test features shape\", test_features_pca.shape)\n",
    "\n",
    "clf_pca = GaussianAD()\n",
    "clf_pca.fit(train_features_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores_pca = clf_pca.decision_function(test_features_pca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_pca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"PCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and separately the low-variance components (all components explaining the remaining 1% of variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative PCA\n",
    "npca_comps = pca.components_[i_comp_thresholds[0]:]\n",
    "\n",
    "train_features_npca = np.matmul(train_features, npca_comps.T)\n",
    "test_features_npca = np.matmul(test_features, npca_comps.T)\n",
    "\n",
    "print(\"NPCA training features shape\", train_features_npca.shape)\n",
    "print(\"NPCA test features shape\", test_features_npca.shape)\n",
    "\n",
    "clf_npca = GaussianAD()\n",
    "clf_npca.fit(train_features_npca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_scores_npca = clf_npca.decision_function(test_features_npca)\n",
    "\n",
    "fpr_img, tpr_img, thresholds_img = roc_curve(ano_gt, ano_scores_npca)\n",
    "auroc_img = auc(fpr_img, tpr_img)\n",
    "\n",
    "print(f\"NPCA reduction, image-wise AUROC: {auroc_img:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, the scores seem to confirm the visual impression that the low variance components do a better job in separating the 'good' and defect images. \n",
    "This is somewhat counterintuitive as one would expect that the the high variance components approximate the training data a lot better. After all, they represent 99% of the variance.\n",
    "It seems like the components used to differentiate individual images of the normal data are different than the components used to differentiate normal from anomalous images. \n",
    "The authors of the Gaussian AD paper made the same observation and hypothesized that this explains why using very general feature extractors like models trained on Imagenet perform a lot better than training or even finetuning a model on the training data.\n",
    "The training data consists only of good images. A model trained on that data learns features to distinguish them. However, as we have seen, those features are less useful in distinguishing normal from anomalous images. Hence, a model trained on this data doesn't learn the necessary features to perform anomaly detection. Even when finetuning on the training data, one risks unlearning those features used to differentiate anomalous images and replacing them with high variance features for normal data only. \n",
    "Therefore, according to this argumentation, it is best to simply use an all-purpose feature extractor with frozen weights for anomaly detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_path = output_path / \"ROC_curve.html\"\n",
    "# save_plot_from_notbook_for_jekyll(p, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We demonstrated a simple approach for image anomaly detection that consists of storing features from the DINOv2 all-purpose feature extractor and fitting a multivariate Gaussian distribution on it. Afterwards, the distance from the Gaussian can be used to detect anomalies in features extracted from test images with a high accuracy. The approach uses only normal data samples and can therefore be easily applied in practice where defect images are hard to get. Furthermore, by analyzing the extracted features further, we observed that feature combinations used to separate anomalous images from normal ones show little variance in the normal training data. This could be an explanation for the higher effectiveness of using fixed feature extractor versus training them on the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
